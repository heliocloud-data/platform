# This file can update the JupyterHub Helm chart's default configuration values.
#
# For reference see the configuration reference and default values, but make
# sure to refer to the Helm chart version of interest to you!
#
# Introduction to YAML:     https://www.youtube.com/watch?v=cdLNKUoMc6c
# Chart config reference:   https://zero-to-jupyterhub.readthedocs.io/en/stable/resources/reference.html
# Chart default values:     https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/HEAD/jupyterhub/values.yaml
# Available chart versions: https://jupyterhub.github.io/helm-chart/
#

rbac:
  enable: true  # Create and use roles and service accounts on an RBAC-enabled cluster.

daskhub:
  rbac:
    create: true  # Create and use roles and service accounts on an RBAC-enabled cluster.

  jupyterhub:
    prePuller:
      hook:
        enabled: true
        nodeSelector:
          hub.jupyter.org/node-purpose: user
      pullProfileListImages: false
      pause:
        image:
          name: registry.k8s.io/pause
      continuous:
        enabled: false

    scheduling:
      userScheduler:
        image:
          name: registry.k8s.io/kube-scheduler
          tag: v1.29.1
        plugins:
          score:
            disabled:
            - name: NodeResourcesBalancedAllocation
            - name: NodeAffinity
            - name: InterPodAffinity
            - name: NodeResourcesFit
            - name: ImageLocality
            enabled:
            - name: NodeAffinity
              weight: 14631
            - name: InterPodAffinity
              weight: 1331
            - name: NodeResourcesFit
              weight: 121
            - name: ImageLocality
              weight: 11

      userPlaceholder:
        image:
          name: registry.k8s.io/pause

    debug:
      enabled: true
    cull:
      enabled: true
      timeout: 1801
      every: 300
    # JupyterHub configuration goes here.
    # See https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/master/jupyterhub/values.yaml
    hub:
      extraEnv:
        EXTRA_PIP_PACKAGES: >-
          boto3
          dask_gatway_server

    singleuser:
      startTimeout: 900
      storage:
        capacity: 100Gi
        extraVolumes:
          - name: dh-helio-efs
            persistentVolumeClaim:
              claimName: efs-persist
          - name: shm-volume
            emptyDir:
              medium: Memory
        extraVolumeMounts:
          - name: dh-helio-efs
            mountPath: /home/jovyan/scratch_space
          - name: dh-helio-efs
            mountPath: /efs
          - name: shm-volume
            mountPath: /dev/shm
      initContainers:
        - name: nfs-fixer
          image: alpine:3
          securityContext:
            runAsUser: 0
          volumeMounts:
          - name: dh-helio-efs
            mountPath: /efs
          command:
          - sh
          - -c
          - (chmod 0775 /efs; chown 1000:100 /efs)
        - name: deploy-science-tutorials
          image: public.ecr.aws/q3h7b4o8/helio-science-tutorials:latest
          securityContext:
            runAsUser: 0
          command:
          - sh
          - -c
          - (cp -R /heliocloud/. /home/jovyan || echo "warning files already exist") && (chown 1000:100 -R /home/jovyan || echo "warning failed to update permissions")
          volumeMounts:
          - name: volume-{username}{servername}
            mountPath: /home/jovyan
        # This initContainer is responsible some basic file cleanup by removing the old
        # mount point (/home/jovyan/efs) which has been replaced with
        # (/home/jovyan/scratch_space).
        - name: remove-old-efs-mount-folder
          image: alpine:3
          securityContext:
            runAsUser: 0
          command:
          - sh
          - -c
          - (rm -rf /home/jovyan/efs || echo "warning unable to delete old mount directory /home/jovyan/efs")
          volumeMounts:
          - name: volume-{username}{servername}
            mountPath: /home/jovyan
      # The following profile list has been tuned for XX.4xlarge instances (16 Core/64GB RAM)
      # and memory limits were selected to allocate the majority of the available RAM whole
      # numbers where "Server", "Large Server", "X-Large Server" and, "The Big Big Server"
      # can support 8, 4, 2 and 1 servers per node respectively.
      #
      # The trick here is to ensure the remaining RAM after hitting that whole number is
      # less than the memory_limit on the smallest server.
      profileList:
      - display_name: "Server"
        description: "Regular notebook server. 8 GB RAM/2 CPU."
        default: true
        kubespawner_override:
          mem_guarantee: 7G
          mem_limit: 7G
          cpu_guarantee: 1
          cpu_limit: 2
      - display_name: "Large Server"
        description: "Large notebook server. 16GB RAM/4 CPU."
        kubespawner_override:
          mem_guarantee: 14G
          mem_limit: 14G
          cpu_guarantee: 3
          cpu_limit: 4
      - display_name: "X-Large Server"
        description: "Large notebook server. 32 GB RAM/8 CPU."
        kubespawner_override:
          mem_guarantee: 28G
          mem_limit: 28G
          cpu_guarantee: 6
          cpu_limit: 8
      - display_name: "The Big Big Server"
        description: "Use resources like you mean it. 64GB RAM/16 CPU reserved."
        kubespawner_override:
          mem_guarantee: 56G
          mem_limit: 56G
          cpu_guarantee: 12
          cpu_limit: 16
      - display_name: "GPU Server"
        description: "Notebook server with access to an NVidia T4 GPU. Up to 16GB RAM/4 CPU."
        kubespawner_override:
          mem_guarantee: 14G
          mem_limit: 14G
          cpu_guarantee: 3
          cpu_limit: 4
          image: {{ config['daskhub']['ML_DOCKER_LOCATION'] }}:{{ config['daskhub']['ML_DOCKER_VERSION'] }}
          environment: {'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility'}
          node_selector:
            nvidia.com/gpu: "true"
          tolerations: [{'key': 'nvidia.com/gpu','operator': 'Equal','value': 'true','effect': 'NoSchedule'},{'key': 'hub.jupyter.org/dedicated','operator': 'Equal','value': 'user','effect': 'NoSchedule'}]
          extra_resource_limits: {"nvidia.com/gpu": "1"}
      - display_name: "GPU Server - Large"
        description: "Notebook server with access to an NVidia T4 GPU. Up to 32GB RAM/8 CPU."
        kubespawner_override:
          mem_guarantee: 28G
          mem_limit: 28G
          cpu_guarantee: 6
          cpu_limit: 8
          image: {{ config['daskhub']['ML_DOCKER_LOCATION'] }}:{{ config['daskhub']['ML_DOCKER_VERSION'] }}
          environment: {'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility'}
          node_selector:
            nvidia.com/gpu: "true"
          tolerations: [{'key': 'nvidia.com/gpu','operator': 'Equal','value': 'true','effect': 'NoSchedule'},{'key': 'hub.jupyter.org/dedicated','operator': 'Equal','value': 'user','effect': 'NoSchedule'}]
          extra_resource_limits: {"nvidia.com/gpu": "1"}
      - display_name: "GPU Server - X-Large"
        description: "Notebook server with access to an NVidia T4 GPU. Up to 64GB RAM/16 CPU."
        kubespawner_override:
          mem_guarantee: 56G
          mem_limit: 56G
          cpu_guarantee: 12
          cpu_limit: 16
          image: {{ config['daskhub']['ML_DOCKER_LOCATION'] }}:{{ config['daskhub']['ML_DOCKER_VERSION'] }}
          environment: {'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility'}
          node_selector:
            nvidia.com/gpu: "true"
          tolerations: [{'key': 'nvidia.com/gpu','operator': 'Equal','value': 'true','effect': 'NoSchedule'},{'key': 'hub.jupyter.org/dedicated','operator': 'Equal','value': 'user','effect': 'NoSchedule'}]
          extra_resource_limits: {"nvidia.com/gpu": "1"}
      image:
        name: {{ config['daskhub']['GENERIC_DOCKER_LOCATION'] }}
        tag: "{{ config['daskhub']['GENERIC_DOCKER_VERSION'] }}"
      cpu:
        guarantee: 1
        limit: 4
      memory:
        guarantee: 1G
        limit: 4G

      # Specifically, this '{JUPYTER_IMAGE_SPEC}' was causing jupyterhub to crash When
      # attempting to render the string in python, explicitly resolving it appeared to
      # resolve the issue.
      #
      # Here's what the values are in JHUAPL's staging environment:
      #   DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE={JUPYTER_IMAGE_SPEC}
      #   JUPYTER_IMAGE_SPEC=public.ecr.aws/q3h7b4o8/heliocloud/helio-notebook:2023.12.06

      extraEnv:
        DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: "{{ config['daskhub']['GENERIC_DOCKER_LOCATION'] }}:{{ config['daskhub']['GENERIC_DOCKER_VERSION'] }}"

      serviceAccountName: helio-dh-role
      defaultUrl: "/lab"  # Use jupyterlab by default.
    proxy:
      chp:
        nodeSelector:
          lifecycle: OnDemand
      traefik:
        image:
          name: traefik
          tag: "v2.4.11"

  dask-gateway:
    enabled: true  # Enabling dask-gateway will install Dask Gateway as a dependency.
    # Futher Dask Gateway configuration goes here
    # See https://github.com/dask/dask-gateway/blob/master/resources/helm/dask-gateway/values.yaml

    gateway:
      prefix: "/services/dask-gateway"  # Users connect to the Gateway through the JupyterHub service.

      auth:
        type: jupyterhub  # Use JupyterHub to authenticate with Dask Gateway

    #readinessProbe:
        # Enables the readinessProbe.
        #enabled: true
        # Configures the readinessProbe.
        #initialDelaySeconds: 5
        #timeoutSeconds: 2
        #periodSeconds: 10
        #failureThreshold: 3

      extraConfig:
        optionHandler: |
          from dask_gateway_server.options import Options, Integer, Float, String
          def option_handler(options, user):
            extra_annotations = {
                    "hub.jupyter.org/username": user.name,
            }

            extra_labels = {
                    "hub.jupyter.org/username": user.name,
            }

            if ":" not in options.image:
                raise ValueError("When specifying an image you must also provide a tag")

            return {
                "image": options.image,
                "worker_cores": options.worker_cores,
                "worker_memory": int(options.worker_memory * 2 ** 30),
                "scheduler_extra_pod_annotations": extra_annotations,
                "worker_extra_pod_annotations": extra_annotations,
                "scheduler_extra_pod_labels": extra_labels,
                "worker_extra_pod_labels": extra_labels,
            }
          c.Backend.cluster_options = Options(
            String("image", default="{{ config['daskhub']['GENERIC_DOCKER_LOCATION'] }}:{{ config['daskhub']['GENERIC_DOCKER_VERSION'] }}", label="Image"),
            Integer("worker_cores", default=1, min=1, max=4, label="Worker Cores"),
            Float("worker_memory", default=1, min=1, max=8, label="Worker Memory (GiB)"),
            handler=option_handler,
          )
        idle: |
          # timeout after 30 minutes of inactivity
          c.KubeClusterConfig.idle_timeout = 1800

    traefik:
      service:
        type: ClusterIP  # Access Dask Gateway through JupyterHub. To access the Gateway from outside JupyterHub, this must be changed to a `LoadBalancer`.

  dask-kubernetes:
    # Use dask-kubernetes, rather than Dask Gateway, for creating Dask Clusters.
    # Enabling this also requires
    # 1. Setting `jupyterhub.singleuser.serviceAccountName: daskkubernetes`.
    # 2. Ensuring that `dask-kubernetes` is in your singleuser environment.
    enabled: false

