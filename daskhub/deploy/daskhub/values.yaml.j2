# This file can update the JupyterHub Helm chart's default configuration values.
#
# For reference see the configuration reference and default values, but make
# sure to refer to the Helm chart version of interest to you!
#
# Introduction to YAML:     https://www.youtube.com/watch?v=cdLNKUoMc6c
# Chart config reference:   https://zero-to-jupyterhub.readthedocs.io/en/stable/resources/reference.html
# Chart default values:     https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/HEAD/jupyterhub/values.yaml
# Available chart versions: https://jupyterhub.github.io/helm-chart/
#

rbac:
  enable: true  # Create and use roles and service accounts on an RBAC-enabled cluster.

daskhub:
  rbac:
    create: true  # Create and use roles and service accounts on an RBAC-enabled cluster.

  jupyterhub:
    # Performance-wise, the pre-puller should be completely disabled, node groups
    # with shared user servers are inheriently locked to a single (docker) image
    # so there's not really a benifit as scheduling of the first user's server
    # will pull the image at that moment anyway.
    #
    # In the event that we choose to support multiple docker runtimes per shared
    # server OR we re-configure the autoscaler to be less reactive, we might
    # revisit this setting.
    prePuller:
      hook:
        enabled: false
      pullProfileListImages: false
      pause:
        image:
          name: registry.k8s.io/pause
      continuous:
        enabled: false

    scheduling:
      userPods:
        nodeAffinity:
          matchNodePurpose: require

      userScheduler:
        image:
          name: registry.k8s.io/kube-scheduler
          tag: v1.31.2
        plugins:
          score:
            disabled:
            - name: NodeResourcesBalancedAllocation
            - name: NodeAffinity
            - name: InterPodAffinity
            - name: NodeResourcesFit
            - name: ImageLocality
            enabled:
            - name: NodeAffinity
              weight: 14631
            - name: InterPodAffinity
              weight: 1331
            - name: NodeResourcesFit
              weight: 121
            - name: ImageLocality
              weight: 11

      userPlaceholder:
        image:
          name: registry.k8s.io/pause

    debug:
      enabled: true
    cull:
      enabled: true
      timeout: 1801
      every: 300
    # JupyterHub configuration goes here.
    # See https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/master/jupyterhub/values.yaml
    hub:
      extraEnv:
        EXTRA_PIP_PACKAGES: >-
          boto3
          dask_gatway_server

    singleuser:
      startTimeout: 900
      storage:
        capacity: 100Gi
        extraVolumes:
          - name: dh-helio-efs
            persistentVolumeClaim:
              claimName: efs-persist
          - name: shm-volume
            emptyDir:
              medium: Memory
        extraVolumeMounts:
          - name: dh-helio-efs
            mountPath: /home/jovyan/scratch_space
          - name: dh-helio-efs
            mountPath: /efs
          - name: shm-volume
            mountPath: /dev/shm
      initContainers:
        - name: nfs-fixer
          image: alpine:3
          securityContext:
            runAsUser: 0
          volumeMounts:
          - name: dh-helio-efs
            mountPath: /efs
          command:
          - sh
          - -c
          - (chmod 0775 /efs; chown 1000:100 /efs)
        - name: deploy-science-tutorials
          image: public.ecr.aws/q3h7b4o8/helio-science-tutorials:latest
          securityContext:
            runAsUser: 0
          command:
          - sh
          - -c
          - (cp -R /heliocloud/. /home/jovyan || echo "warning files already exist") && (chown 1000:100 -R /home/jovyan || echo "warning failed to update permissions")
          volumeMounts:
          - name: volume-{username}{servername}
            mountPath: /home/jovyan
        # This initContainer is responsible some basic file cleanup by removing the old
        # mount point (/home/jovyan/efs) which has been replaced with
        # (/home/jovyan/scratch_space).
        - name: remove-old-efs-mount-folder
          image: alpine:3
          securityContext:
            runAsUser: 0
          command:
          - sh
          - -c
          - (rm -rf /home/jovyan/efs || echo "warning unable to delete old mount directory /home/jovyan/efs")
          volumeMounts:
          - name: volume-{username}{servername}
            mountPath: /home/jovyan
      # The following profile list has been tuned for XX.4xlarge instances (16 Core/64GB RAM)
      # and memory limits were selected to allocate the majority of the available RAM whole
      # numbers where "Server", "Large Server", "X-Large Server" and, "The Big Big Server"
      # can support 8, 4, 2 and 1 servers per node respectively.
      #
      # The trick here is to ensure the remaining RAM after hitting that whole number is
      # less than the memory_limit on the smallest server.
      profileList:
      - display_name: "Server"
        description: "Regular notebook server. 8 GB RAM/2 CPU. <b>$0.04/hr</b>"
        default: true
        kubespawner_override:
          mem_guarantee: 7G
          mem_limit: 7G
          cpu_guarantee: 1
          cpu_limit: 2
          node_selector:
            nvidia.com/gpu: "false"
          tolerations:
            - key: 'hub.jupyter.org/dedicated'
              operator: 'Equal'
              value: 'user'
              effect: 'NoSchedule'
      - display_name: "The Big Big Server"
        description: "Use resources like you mean it. 64GB RAM/16 CPU reserved. <b>$0.32/hr</b>"
        kubespawner_override:
          mem_guarantee: 56G
          mem_limit: 56G
          cpu_guarantee: 12
          cpu_limit: 16
          node_selector:
            nvidia.com/gpu: "false"
            heliocloud.org/instance-type: "4xlarge"
          tolerations:
            - key: 'hub.jupyter.org/dedicated'
              operator: 'Equal'
              value: 'big-user'
              effect: 'NoSchedule'
      - display_name: "GPU Server"
        description: "Notebook server with access to an NVidia T4 GPU. Up to 16GB RAM/4 CPU. <b>$0.10/hr</b>"
        kubespawner_override:
          mem_guarantee: 14G
          mem_limit: 14G
          cpu_guarantee: 3
          cpu_limit: 4
          image: {{ config['daskhub']['ML_DOCKER_LOCATION'] }}:{{ config['daskhub']['ML_DOCKER_VERSION'] }}
          environment: {'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility'}
          node_selector:
            nvidia.com/gpu: "true"
            heliocloud.org/instance-type: "xlarge"
          tolerations:
            - key: 'nvidia.com/gpu'
              operator: 'Equal'
              value: 'true'
              effect: 'NoSchedule'
            - key: 'hub.jupyter.org/dedicated'
              operator: 'Equal'
              value: 'user'
              effect: 'NoSchedule'
          extra_resource_limits: {"nvidia.com/gpu": "1"}
      - display_name: "GPU Server - X-Large"
        description: "Notebook server with access to an NVidia T4 GPU. Up to 64GB RAM/16 CPU. <b>$0.40/hr</b>"
        kubespawner_override:
          mem_guarantee: 56G
          mem_limit: 56G
          cpu_guarantee: 12
          cpu_limit: 16
          image: {{ config['daskhub']['ML_DOCKER_LOCATION'] }}:{{ config['daskhub']['ML_DOCKER_VERSION'] }}
          environment: {'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility'}
          node_selector:
            nvidia.com/gpu: "true"
            heliocloud.org/instance-type: "4xlarge"
          tolerations:
            - key: 'nvidia.com/gpu'
              operator: 'Equal'
              value: 'true'
              effect: 'NoSchedule'
            - key: 'hub.jupyter.org/dedicated'
              operator: 'Equal'
              value: 'user'
              effect: 'NoSchedule'
          extra_resource_limits: {"nvidia.com/gpu": "1"}
      image:
        name: {{ config['daskhub']['GENERIC_DOCKER_LOCATION'] }}
        tag: "{{ config['daskhub']['GENERIC_DOCKER_VERSION'] }}"
      cpu:
        guarantee: 1
        limit: 4
      memory:
        guarantee: 1G
        limit: 4G

      # Specifically, this '{JUPYTER_IMAGE_SPEC}' was causing jupyterhub to crash When
      # attempting to render the string in python, explicitly resolving it appeared to
      # resolve the issue.
      #
      # Here's what the values are in JHUAPL's staging environment:
      #   DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE={JUPYTER_IMAGE_SPEC}
      #   JUPYTER_IMAGE_SPEC=public.ecr.aws/q3h7b4o8/heliocloud/helio-notebook:2024.09.13

      extraEnv:
        DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: "{{ config['daskhub']['GENERIC_DOCKER_LOCATION'] }}:{{ config['daskhub']['GENERIC_DOCKER_VERSION'] }}"

      serviceAccountName: helio-dh-role
      defaultUrl: "/lab"  # Use jupyterlab by default.
    proxy:
      chp:
        nodeSelector:
          lifecycle: OnDemand
      traefik:
        image:
          name: traefik
          tag: "v2.4.11"

  dask-gateway:
    enabled: true  # Enabling dask-gateway will install Dask Gateway as a dependency.
    # Futher Dask Gateway configuration goes here
    # See https://github.com/dask/dask-gateway/blob/master/resources/helm/dask-gateway/values.yaml

    gateway:
      prefix: "/services/dask-gateway"  # Users connect to the Gateway through the JupyterHub service.

      auth:
        type: jupyterhub  # Use JupyterHub to authenticate with Dask Gateway

      # These settings ensure that we properly select the node group with
      # Ec2Spot.  It might be better to switch to taint/toleration, not sure.
      #
      # See:
      #   https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
      #   https://discourse.jupyter.org/t/tailoring-spawn-options-and-server-configuration-to-certain-users/8449
      backend:
        scheduler:
          extraPodConfig:
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: hub.jupyter.org/node-purpose
                      operator: In
                      values:
                      - core
                      - user
                      - dask
        worker:
          extraPodConfig:
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: lifecycle
                      operator: In
                      values:
                      - Ec2Spot

      extraConfig:
        optionHandler: |
          from dask_gateway_server.options import Options, Integer, Float, String, Select
          import copy

          extra_pod_affinity_config = {
            "nodeAffinity": {
              "requiredDuringSchedulingIgnoredDuringExecution": {
                "nodeSelectorTerms": [{
                  "matchExpressions": [{
                    "key": "heliocloud.org/dask-worker",
                    "operator": "In",
                    "values": [
                      "true",
                    ]
                  }]
                }]
              }
            }
          }

          # Enumerate all the instance profiles that we want to support dask scheduling
          # for.  We'll need corresponding node groups in the cluster.  We control this
          # via the cluster-config.yaml.
          profiles = {
            "default": {
              "extra_node_selectors": {
                "nvidia.com/gpu": "false",
                "heliocloud.org/dask-worker": "true",
              },

              "extra_node_affinity": [{
                "matchExpressions": [{
                  "key": "nvidia.com/gpu",
                  "operator": "In",
                  "values": [
                    "false",
                  ]
                }]
              }],

              "scheduler_extra_pod_config": {
                "affinity": extra_pod_affinity_config,
                "tolerations": [{
                  "key": "heliocloud.org/dask-worker-profile",
                  "operator": "Equal",
                  "value": "default",
                  "effect": "NoSchedule",
                }]
              },
              "worker_extra_pod_config": {
                "affinity": extra_pod_affinity_config,
                "tolerations": [{
                  "key": "heliocloud.org/dask-worker-profile",
                  "operator": "Equal",
                  "value": "default",
                  "effect": "NoSchedule",
                }]
              },
              "environment": {},
            },
            "gpu-xlarge": {
              "overrides": {
                "worker_cores": 3.5,
                "worker_memory": "14G",
              },

              "extra_node_selectors": {
                "nvidia.com/gpu": "true",
                "heliocloud.org/dask-worker": "true",
              },

              "extra_node_affinity": [{
                "matchExpressions": [{
                  "key": "nvidia.com/gpu",
                  "operator": "In",
                  "values": [
                    "true",
                  ]
                }]
              }],

              "scheduler_extra_pod_config": {
                "affinity": extra_pod_affinity_config,
                "tolerations": [{
                  "key": "heliocloud.org/dask-worker-profile",
                  "operator": "Equal",
                  "value": "gpu-xlarge",
                  "effect": "NoSchedule",
                }]
              },
              "worker_extra_pod_config": {
                "affinity": extra_pod_affinity_config,
                "tolerations": [{
                  "key": "heliocloud.org/dask-worker-profile",
                  "operator": "Equal",
                  "value": "gpu-xlarge",
                  "effect": "NoSchedule",
                }]
              },
              "environment": {
                "NVIDIA_DRIVER_CAPABILITIES": "compute,utility",
              },
            }
          }

          def get_extra_pod_config_from_profile(options, user, pod_config_key):
            extra_toleration_user_config = {
              "key": "hub.jupyter.org/username",
              "operator": "Equal",
              "value": user.name,
              "effect": "NoSchedule",
            }

            extra_pod_config = copy.deepcopy(profiles[options.profile][pod_config_key])

            if 'tolerations' not in extra_pod_config:
              extra_pod_config['tolerations'] = []

            # Inject tolerations associated w/ this user.
            extra_pod_config['tolerations'].append(extra_toleration_user_config)

            # Apply any additional node selectors for the given profile
            # configuration
            if 'extra_node_selectors' in profiles[options.profile]:
              extra_pod_config['nodeSelector'] = profiles[options.profile]['extra_node_selectors']

            # Apply any additional node affinity for the given profile
            # configuration
            if 'extra_node_affinity' in profiles[options.profile] and len(profiles[options.profile]['extra_node_affinity']) > 0:
              if 'affinity' not in extra_pod_config:
                extra_pod_config['affinity'] = {}
              if 'nodeAffinity' not in extra_pod_config['affinity']:
                extra_pod_config['affinity']['nodeAffinity'] = {}
              if 'requiredDuringSchedulingIgnoredDuringExecution' not in extra_pod_config['affinity']['nodeAffinity']:
                extra_pod_config['affinity']['nodeAffinity']['requiredDuringSchedulingIgnoredDuringExecution'] = {}
              if 'nodeSelectorTerms' not in extra_pod_config['affinity']['nodeAffinity']['requiredDuringSchedulingIgnoredDuringExecution']:
                extra_pod_config['affinity']['nodeAffinity']['requiredDuringSchedulingIgnoredDuringExecution']['nodeSelectorTerms'] = []
              for extra_node_selector in profiles[options.profile]['extra_node_affinity']:
                extra_pod_config['affinity']['nodeAffinity']['requiredDuringSchedulingIgnoredDuringExecution']['nodeSelectorTerms'].append(extra_node_selector)

            return extra_pod_config

          def option_handler(options, user):
            environment = copy.deepcopy(profiles[options.profile]['environment'])

            worker_extra_pod_config = get_extra_pod_config_from_profile(options, user, 'worker_extra_pod_config')
            scheduler_extra_pod_config = get_extra_pod_config_from_profile(options, user, 'scheduler_extra_pod_config')

            extra_annotations = {
              "hub.jupyter.org/username": user.name,
            }

            extra_labels = {
              "hub.jupyter.org/username": user.name,
            }

            if ":" not in options.image:
              raise ValueError("When specifying an image you must also provide a tag")

            return {
              "image": options.image,
              "worker_cores": profiles[options.profile].get("overrides",{}).get("worker_cores",options.worker_cores),
              "worker_memory": profiles[options.profile].get("overrides",{}).get("worker_memory", int(options.worker_memory * 2 ** 30)),
              "scheduler_extra_pod_annotations": extra_annotations,
              "worker_extra_pod_annotations": extra_annotations,
              "scheduler_extra_pod_labels": extra_labels,
              "worker_extra_pod_labels": extra_labels,
              "environment": environment,
              "worker_extra_pod_config": worker_extra_pod_config,
              "scheduler_extra_pod_config": scheduler_extra_pod_config,
            }
          c.Backend.cluster_options = Options(
            String("image", default="{{ config['daskhub']['GENERIC_DOCKER_LOCATION'] }}:{{ config['daskhub']['GENERIC_DOCKER_VERSION'] }}", label="Image"),
            Integer("worker_cores", default=1, min=1, max=4, label="Worker Cores"),
            Float("worker_memory", default=1, min=1, max=8, label="Worker Memory (GiB)"),
            Select("profile", list(profiles.keys()), default="default", label="Profile"),
            handler=option_handler,
          )
        idle: |
          # timeout after 30 minutes of inactivity
          c.KubeClusterConfig.idle_timeout = 1800

    traefik:
      service:
        type: ClusterIP  # Access Dask Gateway through JupyterHub. To access the Gateway from outside JupyterHub, this must be changed to a `LoadBalancer`.

  dask-kubernetes:
    # Use dask-kubernetes, rather than Dask Gateway, for creating Dask Clusters.
    # Enabling this also requires
    # 1. Setting `jupyterhub.singleuser.serviceAccountName: daskkubernetes`.
    # 2. Ensuring that `dask-kubernetes` is in your singleuser environment.
    enabled: false

