---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: api-daskhub-dask-gateway
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: autohttps
    heritage: Helm
    release: daskhub
  name: autohttps
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: controller-daskhub-dask-gateway
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: hub
    heritage: Helm
    release: daskhub
  name: hub
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: traefik-daskhub-dask-gateway
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: user-scheduler
    heritage: Helm
    release: daskhub
  name: user-scheduler
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: autohttps
    heritage: Helm
    release: daskhub
  name: autohttps
rules:
  - apiGroups:
      - ''
    resources:
      - secrets
    verbs:
      - get
      - patch
      - list
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: hub
    heritage: Helm
    release: daskhub
  name: hub
rules:
  - apiGroups:
      - ''
    resources:
      - pods
      - persistentvolumeclaims
      - secrets
      - services
    verbs:
      - get
      - watch
      - list
      - create
      - delete
  - apiGroups:
      - ''
    resources:
      - events
    verbs:
      - get
      - watch
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: api-daskhub-dask-gateway
rules:
  - apiGroups:
      - ''
    resources:
      - secrets
    verbs:
      - get
  - apiGroups:
      - gateway.dask.org
    resources:
      - daskclusters
    verbs:
      - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: controller-daskhub-dask-gateway
rules:
  - apiGroups:
      - gateway.dask.org
    resources:
      - daskclusters
      - daskclusters/status
    verbs:
      - '*'
  - apiGroups:
      - traefik.containo.us
    resources:
      - ingressroutes
      - ingressroutetcps
    verbs:
      - get
      - create
      - delete
  - apiGroups:
      - ''
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
      - create
      - delete
  - apiGroups:
      - ''
    resources:
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - secrets
      - services
    verbs:
      - create
      - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: user-scheduler
    heritage: Helm
    kustomization: patch-jupyterhub-user-scheduler-apiVersion
    release: daskhub
  name: daskhub-user-scheduler
rules:
  - apiGroups:
      - ''
      - events.k8s.io
    resources:
      - events
    verbs:
      - create
      - patch
      - update
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
  - apiGroups:
      - coordination.k8s.io
    resourceNames:
      - user-scheduler-lock
    resources:
      - leases
    verbs:
      - get
      - update
  - apiGroups:
      - ''
    resources:
      - endpoints
    verbs:
      - create
  - apiGroups:
      - ''
    resourceNames:
      - user-scheduler-lock
    resources:
      - endpoints
    verbs:
      - get
      - update
  - apiGroups:
      - ''
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - pods
    verbs:
      - delete
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - bindings
      - pods/binding
    verbs:
      - create
  - apiGroups:
      - ''
    resources:
      - pods/status
    verbs:
      - patch
      - update
  - apiGroups:
      - ''
    resources:
      - replicationcontrollers
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
      - extensions
    resources:
      - replicasets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - persistentvolumeclaims
      - persistentvolumes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - authentication.k8s.io
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
  - apiGroups:
      - storage.k8s.io
    resources:
      - csinodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - csidrivers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - csistoragecapacities
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - persistentvolumes
    verbs:
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - persistentvolumeclaims
    verbs:
      - get
      - list
      - patch
      - update
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: traefik-daskhub-dask-gateway
rules:
  - apiGroups:
      - ''
    resources:
      - pods
      - services
      - endpoints
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - traefik.containo.us
    resources:
      - ingressroutes
      - ingressroutetcps
      - ingressrouteudps
      - middlewares
      - middlewaretcps
      - serverstransports
      - tlsoptions
      - tlsstores
      - traefikservices
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: autohttps
    heritage: Helm
    release: daskhub
  name: autohttps
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: autohttps
subjects:
  - apiGroup:
    kind: ServiceAccount
    name: autohttps
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: hub
    heritage: Helm
    release: daskhub
  name: hub
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: hub
subjects:
  - kind: ServiceAccount
    name: hub
    namespace: daskhub
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: api-daskhub-dask-gateway
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: api-daskhub-dask-gateway
subjects:
  - kind: ServiceAccount
    name: api-daskhub-dask-gateway
    namespace: daskhub
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: controller-daskhub-dask-gateway
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: controller-daskhub-dask-gateway
subjects:
  - kind: ServiceAccount
    name: controller-daskhub-dask-gateway
    namespace: daskhub
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: user-scheduler
    heritage: Helm
    release: daskhub
  name: daskhub-user-scheduler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: daskhub-user-scheduler
subjects:
  - kind: ServiceAccount
    name: user-scheduler
    namespace: daskhub
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: traefik-daskhub-dask-gateway
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-daskhub-dask-gateway
subjects:
  - kind: ServiceAccount
    name: traefik-daskhub-dask-gateway
    namespace: daskhub
---
apiVersion: v1
data:
  dask_gateway_config.py: |-
    import os
    import json
    import sys

    # Add this directory to path to support extension modules
    sys.path.insert(0, os.path.dirname(os.path.realpath(__file__)))

    _PROPERTIES = json.loads("{\"gateway\":{\"affinity\":{},\"annotations\":{},\"auth\":{\"custom\":{\"config\":{}},\"jupyterhub\":{},\"kerberos\":{},\"simple\":{},\"type\":\"jupyterhub\"},\"backend\":{\"environment\":{},\"image\":{\"name\":\"ghcr.io/dask/dask-gateway\",\"pullPolicy\":\"IfNotPresent\",\"tag\":\"2022.6.1\"},\"scheduler\":{\"cores\":{},\"extraContainerConfig\":{},\"extraPodConfig\":{},\"memory\":{}},\"worker\":{\"cores\":{},\"extraContainerConfig\":{},\"extraPodConfig\":{},\"memory\":{}}},\"image\":{\"name\":\"ghcr.io/dask/dask-gateway-server\",\"pullPolicy\":\"IfNotPresent\",\"tag\":\"2022.6.1\"},\"imagePullSecrets\":[],\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"timeoutSeconds\":2},\"loglevel\":\"INFO\",\"nodeSelector\":{},\"prefix\":\"/services/dask-gateway\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"timeoutSeconds\":2},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{}},\"tolerations\":[]}}")

    def get_property(key, default=None):
        """Read a property from the configured helm values."""
        value = _PROPERTIES
        for key2 in key.split("."):
            if not isinstance(value, dict) or key2 not in value:
                return default
            value = value[key2]
        return value

    c.DaskGateway.log_level = "INFO"

    # Configure addresses
    c.DaskGateway.address = ":8000"
    c.KubeBackend.api_url = 'http://api-daskhub-dask-gateway.daskhub:8000/api'

    # Configure the backend
    c.DaskGateway.backend_class = "dask_gateway_server.backends.kubernetes.KubeBackend"
    c.KubeBackend.gateway_instance = "daskhub-dask-gateway"

    # Configure the dask cluster image
    image_name = get_property("gateway.backend.image.name")
    if image_name:
        image_tag = get_property("gateway.backend.image.tag")
        c.KubeClusterConfig.image = (
            "%s:%s" % (image_name, image_tag) if image_tag else image_name
        )

    # Forward dask cluster configuration
    for field, prop_name in [
        # Scheduler config
        ("scheduler_cores", "scheduler.cores.request"),
        ("scheduler_cores_limit", "scheduler.cores.limit"),
        ("scheduler_memory", "scheduler.memory.request"),
        ("scheduler_memory_limit", "scheduler.memory.limit"),
        ("scheduler_extra_container_config", "scheduler.extraContainerConfig"),
        ("scheduler_extra_pod_config", "scheduler.extraPodConfig"),
        # Worker config
        ("worker_cores", "worker.cores.request"),
        ("worker_cores_limit", "worker.cores.limit"),
        ("worker_memory", "worker.memory.request"),
        ("worker_memory_limit", "worker.memory.limit"),
        ("worker_threads", "worker.threads"),
        ("worker_extra_container_config", "worker.extraContainerConfig"),
        ("worker_extra_pod_config", "worker.extraPodConfig"),
        # Additional fields
        ("image_pull_policy", "image.pullPolicy"),
        ("environment", "environment"),
        ("namespace", "namespace"),
    ]:
        value = get_property("gateway.backend." + prop_name)
        if value is not None:
            setattr(c.KubeClusterConfig, field, value)

    # Authentication
    auth_type = get_property("gateway.auth.type")
    if auth_type == "simple":
        c.DaskGateway.authenticator_class = "dask_gateway_server.auth.SimpleAuthenticator"
        password = get_property("gateway.auth.simple.password")
        if password is not None:
            c.SimpleAuthenticator.password = password
    elif auth_type == "kerberos":
        c.DaskGateway.authenticator_class = "dask_gateway_server.auth.KerberosAuthenticator"
        keytab = get_property("gateway.auth.kerberos.keytab")
        if keytab is not None:
            c.KerberosAuthenticator.keytab = keytab
    elif auth_type == "jupyterhub":
        c.DaskGateway.authenticator_class = "dask_gateway_server.auth.JupyterHubAuthenticator"
        api_url = get_property("gateway.auth.jupyterhub.apiUrl")
        if api_url is None:
            try:
                api_url = "http://{HUB_SERVICE_HOST}:{HUB_SERVICE_PORT}/hub/api".format(**os.environ)
            except Exception:
                raise ValueError(
                    "Failed to infer JupyterHub API url from environment, "
                    "please specify `gateway.auth.jupyterhub.apiUrl` in "
                    "your config file"
                )
        c.DaskGateway.JupyterHubAuthenticator.jupyterhub_api_url = api_url
    elif auth_type == "custom":
        auth_cls = get_property("gateway.auth.custom.class")
        c.DaskGateway.authenticator_class = auth_cls
        auth_cls_name = auth_cls.rsplit('.', 1)[-1]
        auth_config = c[auth_cls_name]
        auth_config.update(get_property("gateway.auth.custom.config") or {})
    else:
        raise ValueError("Unknown authenticator type %r" % auth_type)

    # From gateway.extraConfig.idle
    # timeout after 30 minutes of inactivity
    c.KubeClusterConfig.idle_timeout = 1800

    # From gateway.extraConfig.optionHandler
    from dask_gateway_server.options import Options, Integer, Float, String
    def option_handler(options, user):
      extra_annotations = {
              "hub.jupyter.org/username": user.name,
      }

      extra_labels = {
              "hub.jupyter.org/username": user.name,
      }

      if ":" not in options.image:
          raise ValueError("When specifying an image you must also provide a tag")

      return {
          "image": options.image,
          "worker_cores": options.worker_cores,
          "worker_memory": int(options.worker_memory * 2 ** 30),
          "scheduler_extra_pod_annotations": extra_annotations,
          "worker_extra_pod_annotations": extra_annotations,
          "scheduler_extra_pod_labels": extra_labels,
          "worker_extra_pod_labels": extra_labels,
      }
    c.Backend.cluster_options = Options(
      String("image", default="<GENERIC_DOCKER_LOCATION>:<GENERIC_DOCKER_VERSION>", label="Image"),
      Integer("worker_cores", default=1, min=1, max=4, label="Worker Cores"),
      Float("worker_memory", default=1, min=1, max=8, label="Worker Memory (GiB)"),
      handler=option_handler,
    )
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: api-daskhub-dask-gateway
---
apiVersion: v1
data:
  dynamic.yaml: |
    http:
      middlewares:
        hsts:
          headers:
            stsIncludeSubdomains: false
            stsPreload: false
            stsSeconds: 15724800
        redirect:
          redirectScheme:
            permanent: true
            scheme: https
        scheme:
          headers:
            customRequestHeaders:
              X-Scheme: https
      routers:
        default:
          entrypoints:
          - https
          middlewares:
          - hsts
          - scheme
          rule: PathPrefix(`/`)
          service: default
          tls:
            certResolver: default
            domains:
            - main: <INSERT_HOST_NAME>
            options: default
        insecure:
          entrypoints:
          - http
          middlewares:
          - redirect
          rule: PathPrefix(`/`)
          service: default
      services:
        default:
          loadBalancer:
            servers:
            - url: http://proxy-http:8000/
    tls:
      options:
        default:
          cipherSuites:
          - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
          - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
          - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
          - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
          - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
          - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          minVersion: VersionTLS12
          sniStrict: true
  traefik.yaml: |
    accessLog:
      fields:
        headers:
          names:
            Authorization: redacted
            Cookie: redacted
            Set-Cookie: redacted
            X-Xsrftoken: redacted
      filters:
        statusCodes:
        - 500-599
    certificatesResolvers:
      default:
        acme:
          caServer: https://acme-v02.api.letsencrypt.org/directory
          email: <INSERT_CONTACT_EMAIL>
          httpChallenge:
            entryPoint: http
          storage: /etc/acme/acme.json
    entryPoints:
      http:
        address: :8080
      https:
        address: :8443
        transport:
          respondingTimeouts:
            idleTimeout: 10m0s
    log:
      level: DEBUG
    providers:
      file:
        filename: /etc/traefik/dynamic.yaml
kind: ConfigMap
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: autohttps
    heritage: Helm
    release: daskhub
  name: autohttps
---
apiVersion: v1
data:
  dask_gateway_config.py: |-
    # Configure addresses
    c.KubeController.address = ":8000"
    c.KubeController.api_url = 'http://api-daskhub-dask-gateway.daskhub:8000/api'
    c.KubeController.gateway_instance = 'daskhub-dask-gateway'
    c.KubeController.proxy_prefix = "/services/dask-gateway"
    c.KubeController.proxy_web_middlewares = [
      {"name": 'clusters-prefix-daskhub-dask-gateway',
      "namespace": 'daskhub'}
    ]
    c.KubeController.log_level = "INFO"
    c.KubeController.completed_cluster_max_age = 86400
    c.KubeController.completed_cluster_cleanup_period = 600
    c.KubeController.backoff_base_delay = 0.1
    c.KubeController.backoff_max_delay = 300
    c.KubeController.k8s_api_rate_limit = 50
    c.KubeController.k8s_api_rate_limit_burst = 100
    c.KubeController.proxy_tcp_entrypoint = "web"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: controller-daskhub-dask-gateway
---
apiVersion: v1
data:
  autohttps: autohttps
  continuous-image-puller: continuous-image-puller
  fullname: ''
  fullname-dash: ''
  hook-image-awaiter: hook-image-awaiter
  hook-image-puller: hook-image-puller
  hub: hub
  hub-existing-secret: ''
  hub-existing-secret-or-default: hub
  hub-pvc: hub-db-dir
  image-pull-secret: image-pull-secret
  ingress: jupyterhub
  jupyterhub_config.py: |
    import glob
    import os
    import re
    import sys

    from binascii import a2b_hex

    from tornado.httpclient import AsyncHTTPClient
    from kubernetes import client
    from jupyterhub.utils import url_path_join

    # Make sure that modules placed in the same directory as the jupyterhub config are added to the pythonpath
    configuration_directory = os.path.dirname(os.path.realpath(__file__))
    sys.path.insert(0, configuration_directory)

    from z2jh import (
        get_config,
        set_config_if_not_none,
        get_name,
        get_name_env,
        get_secret_value,
    )


    def camelCaseify(s):
        """convert snake_case to camelCase

        For the common case where some_value is set from someValue
        so we don't have to specify the name twice.
        """
        return re.sub(r"_([a-z])", lambda m: m.group(1).upper(), s)


    # Configure JupyterHub to use the curl backend for making HTTP requests,
    # rather than the pure-python implementations. The default one starts
    # being too slow to make a large number of requests to the proxy API
    # at the rate required.
    AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")

    c.JupyterHub.spawner_class = "kubespawner.KubeSpawner"

    # Connect to a proxy running in a different pod. Note that *_SERVICE_*
    # environment variables are set by Kubernetes for Services
    c.ConfigurableHTTPProxy.api_url = (
        f'http://{get_name("proxy-api")}:{get_name_env("proxy-api", "_SERVICE_PORT")}'
    )
    c.ConfigurableHTTPProxy.should_start = False

    # Do not shut down user pods when hub is restarted
    c.JupyterHub.cleanup_servers = False

    # Check that the proxy has routes appropriately setup
    c.JupyterHub.last_activity_interval = 60

    # Don't wait at all before redirecting a spawning user to the progress page
    c.JupyterHub.tornado_settings = {
        "slow_spawn_timeout": 0,
    }


    # configure the hub db connection
    db_type = get_config("hub.db.type")
    if db_type == "sqlite-pvc":
        c.JupyterHub.db_url = "sqlite:///jupyterhub.sqlite"
    elif db_type == "sqlite-memory":
        c.JupyterHub.db_url = "sqlite://"
    else:
        set_config_if_not_none(c.JupyterHub, "db_url", "hub.db.url")
    db_password = get_secret_value("hub.db.password", None)
    if db_password is not None:
        if db_type == "mysql":
            os.environ["MYSQL_PWD"] = db_password
        elif db_type == "postgres":
            os.environ["PGPASSWORD"] = db_password
        else:
            print(f"Warning: hub.db.password is ignored for hub.db.type={db_type}")


    # c.JupyterHub configuration from Helm chart's configmap
    for trait, cfg_key in (
        ("concurrent_spawn_limit", None),
        ("active_server_limit", None),
        ("base_url", None),
        ("allow_named_servers", None),
        ("named_server_limit_per_user", None),
        ("authenticate_prometheus", None),
        ("redirect_to_server", None),
        ("shutdown_on_logout", None),
        ("template_paths", None),
        ("template_vars", None),
    ):
        if cfg_key is None:
            cfg_key = camelCaseify(trait)
        set_config_if_not_none(c.JupyterHub, trait, "hub." + cfg_key)

    # hub_bind_url configures what the JupyterHub process within the hub pod's
    # container should listen to.
    hub_container_port = 8081
    c.JupyterHub.hub_bind_url = f"http://:{hub_container_port}"

    # hub_connect_url is the URL for connecting to the hub for use by external
    # JupyterHub services such as the proxy. Note that *_SERVICE_* environment
    # variables are set by Kubernetes for Services.
    c.JupyterHub.hub_connect_url = (
        f'http://{get_name("hub")}:{get_name_env("hub", "_SERVICE_PORT")}'
    )

    # implement common labels
    # this duplicates the jupyterhub.commonLabels helper
    common_labels = c.KubeSpawner.common_labels = {}
    common_labels["app"] = get_config(
        "nameOverride",
        default=get_config("Chart.Name", "jupyterhub"),
    )
    common_labels["heritage"] = "jupyterhub"
    chart_name = get_config("Chart.Name")
    chart_version = get_config("Chart.Version")
    if chart_name and chart_version:
        common_labels["chart"] = "{}-{}".format(
            chart_name,
            chart_version.replace("+", "_"),
        )
    release = get_config("Release.Name")
    if release:
        common_labels["release"] = release

    c.KubeSpawner.namespace = os.environ.get("POD_NAMESPACE", "default")

    # Max number of consecutive failures before the Hub restarts itself
    # requires jupyterhub 0.9.2
    set_config_if_not_none(
        c.Spawner,
        "consecutive_failure_limit",
        "hub.consecutiveFailureLimit",
    )

    for trait, cfg_key in (
        ("pod_name_template", None),
        ("start_timeout", None),
        ("image_pull_policy", "image.pullPolicy"),
        # ('image_pull_secrets', 'image.pullSecrets'), # Managed manually below
        ("events_enabled", "events"),
        ("extra_labels", None),
        ("extra_annotations", None),
        ("uid", None),
        ("fs_gid", None),
        ("service_account", "serviceAccountName"),
        ("storage_extra_labels", "storage.extraLabels"),
        # ("tolerations", "extraTolerations"), # Managed manually below
        ("node_selector", None),
        ("node_affinity_required", "extraNodeAffinity.required"),
        ("node_affinity_preferred", "extraNodeAffinity.preferred"),
        ("pod_affinity_required", "extraPodAffinity.required"),
        ("pod_affinity_preferred", "extraPodAffinity.preferred"),
        ("pod_anti_affinity_required", "extraPodAntiAffinity.required"),
        ("pod_anti_affinity_preferred", "extraPodAntiAffinity.preferred"),
        ("lifecycle_hooks", None),
        ("init_containers", None),
        ("extra_containers", None),
        ("mem_limit", "memory.limit"),
        ("mem_guarantee", "memory.guarantee"),
        ("cpu_limit", "cpu.limit"),
        ("cpu_guarantee", "cpu.guarantee"),
        ("extra_resource_limits", "extraResource.limits"),
        ("extra_resource_guarantees", "extraResource.guarantees"),
        ("environment", "extraEnv"),
        ("profile_list", None),
        ("extra_pod_config", None),
    ):
        if cfg_key is None:
            cfg_key = camelCaseify(trait)
        set_config_if_not_none(c.KubeSpawner, trait, "singleuser." + cfg_key)

    image = get_config("singleuser.image.name")
    if image:
        tag = get_config("singleuser.image.tag")
        if tag:
            image = "{}:{}".format(image, tag)

        c.KubeSpawner.image = image

    # Combine imagePullSecret.create (single), imagePullSecrets (list), and
    # singleuser.image.pullSecrets (list).
    image_pull_secrets = []
    if get_config("imagePullSecret.automaticReferenceInjection") and get_config(
        "imagePullSecret.create"
    ):
        image_pull_secrets.append(get_name("image-pull-secret"))
    if get_config("imagePullSecrets"):
        image_pull_secrets.extend(get_config("imagePullSecrets"))
    if get_config("singleuser.image.pullSecrets"):
        image_pull_secrets.extend(get_config("singleuser.image.pullSecrets"))
    if image_pull_secrets:
        c.KubeSpawner.image_pull_secrets = image_pull_secrets

    # scheduling:
    if get_config("scheduling.userScheduler.enabled"):
        c.KubeSpawner.scheduler_name = get_name("user-scheduler")
    if get_config("scheduling.podPriority.enabled"):
        c.KubeSpawner.priority_class_name = get_name("priority")

    # add node-purpose affinity
    match_node_purpose = get_config("scheduling.userPods.nodeAffinity.matchNodePurpose")
    if match_node_purpose:
        node_selector = dict(
            matchExpressions=[
                dict(
                    key="hub.jupyter.org/node-purpose",
                    operator="In",
                    values=["user"],
                )
            ],
        )
        if match_node_purpose == "prefer":
            c.KubeSpawner.node_affinity_preferred.append(
                dict(
                    weight=100,
                    preference=node_selector,
                ),
            )
        elif match_node_purpose == "require":
            c.KubeSpawner.node_affinity_required.append(node_selector)
        elif match_node_purpose == "ignore":
            pass
        else:
            raise ValueError(
                "Unrecognized value for matchNodePurpose: %r" % match_node_purpose
            )

    # Combine the common tolerations for user pods with singleuser tolerations
    scheduling_user_pods_tolerations = get_config("scheduling.userPods.tolerations", [])
    singleuser_extra_tolerations = get_config("singleuser.extraTolerations", [])
    tolerations = scheduling_user_pods_tolerations + singleuser_extra_tolerations
    if tolerations:
        c.KubeSpawner.tolerations = tolerations

    # Configure dynamically provisioning pvc
    storage_type = get_config("singleuser.storage.type")
    if storage_type == "dynamic":
        pvc_name_template = get_config("singleuser.storage.dynamic.pvcNameTemplate")
        c.KubeSpawner.pvc_name_template = pvc_name_template
        volume_name_template = get_config("singleuser.storage.dynamic.volumeNameTemplate")
        c.KubeSpawner.storage_pvc_ensure = True
        set_config_if_not_none(
            c.KubeSpawner, "storage_class", "singleuser.storage.dynamic.storageClass"
        )
        set_config_if_not_none(
            c.KubeSpawner,
            "storage_access_modes",
            "singleuser.storage.dynamic.storageAccessModes",
        )
        set_config_if_not_none(
            c.KubeSpawner, "storage_capacity", "singleuser.storage.capacity"
        )

        # Add volumes to singleuser pods
        c.KubeSpawner.volumes = [
            {
                "name": volume_name_template,
                "persistentVolumeClaim": {"claimName": pvc_name_template},
            }
        ]
        c.KubeSpawner.volume_mounts = [
            {
                "mountPath": get_config("singleuser.storage.homeMountPath"),
                "name": volume_name_template,
            }
        ]
    elif storage_type == "static":
        pvc_claim_name = get_config("singleuser.storage.static.pvcName")
        c.KubeSpawner.volumes = [
            {"name": "home", "persistentVolumeClaim": {"claimName": pvc_claim_name}}
        ]

        c.KubeSpawner.volume_mounts = [
            {
                "mountPath": get_config("singleuser.storage.homeMountPath"),
                "name": "home",
                "subPath": get_config("singleuser.storage.static.subPath"),
            }
        ]

    # Inject singleuser.extraFiles as volumes and volumeMounts with data loaded from
    # the dedicated k8s Secret prepared to hold the extraFiles actual content.
    extra_files = get_config("singleuser.extraFiles", {})
    if extra_files:
        volume = {
            "name": "files",
        }
        items = []
        for file_key, file_details in extra_files.items():
            # Each item is a mapping of a key in the k8s Secret to a path in this
            # abstract volume, the goal is to enable us to set the mode /
            # permissions only though so we don't change the mapping.
            item = {
                "key": file_key,
                "path": file_key,
            }
            if "mode" in file_details:
                item["mode"] = file_details["mode"]
            items.append(item)
        volume["secret"] = {
            "secretName": get_name("singleuser"),
            "items": items,
        }
        c.KubeSpawner.volumes.append(volume)

        volume_mounts = []
        for file_key, file_details in extra_files.items():
            volume_mounts.append(
                {
                    "mountPath": file_details["mountPath"],
                    "subPath": file_key,
                    "name": "files",
                }
            )
        c.KubeSpawner.volume_mounts.extend(volume_mounts)

    # Inject extraVolumes / extraVolumeMounts
    c.KubeSpawner.volumes.extend(get_config("singleuser.storage.extraVolumes", []))
    c.KubeSpawner.volume_mounts.extend(
        get_config("singleuser.storage.extraVolumeMounts", [])
    )

    c.JupyterHub.services = []

    if get_config("cull.enabled", False):
        cull_cmd = ["python3", "-m", "jupyterhub_idle_culler"]
        base_url = c.JupyterHub.get("base_url", "/")
        cull_cmd.append("--url=http://localhost:8081" + url_path_join(base_url, "hub/api"))

        cull_timeout = get_config("cull.timeout")
        if cull_timeout:
            cull_cmd.append("--timeout=%s" % cull_timeout)

        cull_every = get_config("cull.every")
        if cull_every:
            cull_cmd.append("--cull-every=%s" % cull_every)

        cull_concurrency = get_config("cull.concurrency")
        if cull_concurrency:
            cull_cmd.append("--concurrency=%s" % cull_concurrency)

        if get_config("cull.users"):
            cull_cmd.append("--cull-users")

        if get_config("cull.removeNamedServers"):
            cull_cmd.append("--remove-named-servers")

        cull_max_age = get_config("cull.maxAge")
        if cull_max_age:
            cull_cmd.append("--max-age=%s" % cull_max_age)

        c.JupyterHub.services.append(
            {
                "name": "cull-idle",
                "admin": True,
                "command": cull_cmd,
            }
        )

    for key, service in get_config("hub.services", {}).items():
        # c.JupyterHub.services is a list of dicts, but
        # hub.services is a dict of dicts to make the config mergable
        service.setdefault("name", key)

        # As the api_token could be exposed in hub.existingSecret, we need to read
        # it it from there or fall back to the chart managed k8s Secret's value.
        service.pop("apiToken", None)
        service["api_token"] = get_secret_value(f"hub.services.{key}.apiToken")

        c.JupyterHub.services.append(service)


    set_config_if_not_none(c.Spawner, "cmd", "singleuser.cmd")
    set_config_if_not_none(c.Spawner, "default_url", "singleuser.defaultUrl")

    cloud_metadata = get_config("singleuser.cloudMetadata", {})

    if cloud_metadata.get("blockWithIptables") == True:
        # Use iptables to block access to cloud metadata by default
        network_tools_image_name = get_config("singleuser.networkTools.image.name")
        network_tools_image_tag = get_config("singleuser.networkTools.image.tag")
        ip_block_container = client.V1Container(
            name="block-cloud-metadata",
            image=f"{network_tools_image_name}:{network_tools_image_tag}",
            command=[
                "iptables",
                "-A",
                "OUTPUT",
                "-d",
                cloud_metadata.get("ip", "169.254.169.254"),
                "-j",
                "DROP",
            ],
            security_context=client.V1SecurityContext(
                privileged=True,
                run_as_user=0,
                capabilities=client.V1Capabilities(add=["NET_ADMIN"]),
            ),
        )

        c.KubeSpawner.init_containers.append(ip_block_container)


    if get_config("debug.enabled", False):
        c.JupyterHub.log_level = "DEBUG"
        c.Spawner.debug = True

    # load /usr/local/etc/jupyterhub/jupyterhub_config.d config files
    config_dir = "/usr/local/etc/jupyterhub/jupyterhub_config.d"
    if os.path.isdir(config_dir):
        for file_path in sorted(glob.glob(f"{config_dir}/*.py")):
            file_name = os.path.basename(file_path)
            print(f"Loading {config_dir} config: {file_name}")
            with open(file_path) as f:
                file_content = f.read()
            # compiling makes debugging easier: https://stackoverflow.com/a/437857
            exec(compile(source=file_content, filename=file_name, mode="exec"))

    # load potentially seeded secrets
    #
    # NOTE: ConfigurableHTTPProxy.auth_token is set through an environment variable
    #       that is set using the chart managed secret.
    c.JupyterHub.cookie_secret = get_secret_value("hub.config.JupyterHub.cookie_secret")
    # NOTE: CryptKeeper.keys should be a list of strings, but we have encoded as a
    #       single string joined with ; in the k8s Secret.
    #
    c.CryptKeeper.keys = get_secret_value("hub.config.CryptKeeper.keys").split(";")

    # load hub.config values, except potentially seeded secrets already loaded
    for app, cfg in get_config("hub.config", {}).items():
        if app == "JupyterHub":
            cfg.pop("proxy_auth_token", None)
            cfg.pop("cookie_secret", None)
            cfg.pop("services", None)
        elif app == "ConfigurableHTTPProxy":
            cfg.pop("auth_token", None)
        elif app == "CryptKeeper":
            cfg.pop("keys", None)
        c[app].update(cfg)

    # execute hub.extraConfig entries
    for key, config_py in sorted(get_config("hub.extraConfig", {}).items()):
        print("Loading extra config: %s" % key)
        exec(config_py)
  priority: daskhub-default-priority
  proxy: proxy
  proxy-api: proxy-api
  proxy-http: proxy-http
  proxy-public: proxy-public
  proxy-public-manual-tls: proxy-public-manual-tls
  proxy-public-tls: proxy-public-tls-acme
  singleuser: singleuser
  user-placeholder: user-placeholder
  user-placeholder-priority: daskhub-user-placeholder-priority
  user-scheduler: daskhub-user-scheduler
  user-scheduler-deploy: user-scheduler
  user-scheduler-lock: user-scheduler-lock
  z2jh.py: |
    """
    Utility methods for use in jupyterhub_config.py and dynamic subconfigs.

    Methods here can be imported by extraConfig in values.yaml
    """
    from collections import Mapping
    from functools import lru_cache
    import os

    import yaml

    # memoize so we only load config once
    @lru_cache()
    def _load_config():
        """Load the Helm chart configuration used to render the Helm templates of
        the chart from a mounted k8s Secret, and merge in values from an optionally
        mounted secret (hub.existingSecret)."""

        cfg = {}
        for source in ("secret/values.yaml", "existing-secret/values.yaml"):
            path = f"/usr/local/etc/jupyterhub/{source}"
            if os.path.exists(path):
                print(f"Loading {path}")
                with open(path) as f:
                    values = yaml.safe_load(f)
                cfg = _merge_dictionaries(cfg, values)
            else:
                print(f"No config at {path}")
        return cfg


    @lru_cache()
    def _get_config_value(key):
        """Load value from the k8s ConfigMap given a key."""

        path = f"/usr/local/etc/jupyterhub/config/{key}"
        if os.path.exists(path):
            with open(path) as f:
                return f.read()
        else:
            raise Exception(f"{path} not found!")


    @lru_cache()
    def get_secret_value(key, default="never-explicitly-set"):
        """Load value from the user managed k8s Secret or the default k8s Secret
        given a key."""

        for source in ("existing-secret", "secret"):
            path = f"/usr/local/etc/jupyterhub/{source}/{key}"
            if os.path.exists(path):
                with open(path) as f:
                    return f.read()
        if default != "never-explicitly-set":
            return default
        raise Exception(f"{key} not found in either k8s Secret!")


    def get_name(name):
        """Returns the fullname of a resource given its short name"""
        return _get_config_value(name)


    def get_name_env(name, suffix=""):
        """Returns the fullname of a resource given its short name along with a
        suffix, converted to uppercase with dashes replaced with underscores. This
        is useful to reference named services associated environment variables, such
        as PROXY_PUBLIC_SERVICE_PORT."""
        env_key = _get_config_value(name) + suffix
        env_key = env_key.upper().replace("-", "_")
        return os.environ[env_key]


    def _merge_dictionaries(a, b):
        """Merge two dictionaries recursively.

        Simplified From https://stackoverflow.com/a/7205107
        """
        merged = a.copy()
        for key in b:
            if key in a:
                if isinstance(a[key], Mapping) and isinstance(b[key], Mapping):
                    merged[key] = _merge_dictionaries(a[key], b[key])
                else:
                    merged[key] = b[key]
            else:
                merged[key] = b[key]
        return merged


    def get_config(key, default=None):
        """
        Find a config item of a given name & return it

        Parses everything as YAML, so lists and dicts are available too

        get_config("a.b.c") returns config['a']['b']['c']
        """
        value = _load_config()
        # resolve path in yaml
        for level in key.split("."):
            if not isinstance(value, dict):
                # a parent is a scalar or null,
                # can't resolve full path
                return default
            if level not in value:
                return default
            else:
                value = value[level]
        return value


    def set_config_if_not_none(cparent, name, key):
        """
        Find a config item of a given name, set the corresponding Jupyter
        configuration item if not None
        """
        data = get_config(key)
        if data is not None:
            setattr(cparent, name, data)
kind: ConfigMap
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: hub
    heritage: Helm
    release: daskhub
  name: hub
---
apiVersion: v1
data:
  config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta1
    kind: KubeSchedulerConfiguration
    leaderElection:
      resourceLock: endpoints
      resourceName: user-scheduler-lock
      resourceNamespace: "daskhub"
    profiles:
      - schedulerName: daskhub-user-scheduler
        plugins:
          score:
            disabled:
            - name: SelectorSpread
            - name: TaintToleration
            - name: PodTopologySpread
            - name: NodeResourcesBalancedAllocation
            - name: NodeResourcesLeastAllocated
            - name: NodePreferAvoidPods
            - name: NodeAffinity
            - name: InterPodAffinity
            - name: ImageLocality
            enabled:
            - name: NodePreferAvoidPods
              weight: 161051
            - name: NodeAffinity
              weight: 14631
            - name: InterPodAffinity
              weight: 1331
            - name: NodeResourcesMostAllocated
              weight: 121
            - name: ImageLocality
              weight: 11
kind: ConfigMap
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: user-scheduler
    heritage: Helm
    release: daskhub
  name: user-scheduler
---
apiVersion: v1
data:
  jupyterhub-api-token: PElOU0VSVF9BUElfS0VZMj4=
kind: Secret
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: api-daskhub-dask-gateway
type: Opaque
---
apiVersion: v1
data:
  hub.services.dask-gateway.apiToken: PElOU0VSVF9BUElfS0VZMj4=
kind: Secret
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: hub
    heritage: Helm
    release: daskhub
  name: hub
type: Opaque
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: api-daskhub-dask-gateway
spec:
  ports:
    - port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    app.kubernetes.io/component: gateway
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/name: dask-gateway
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/path: /hub/metrics
    prometheus.io/port: '8081'
    prometheus.io/scrape: 'true'
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: hub
    heritage: Helm
    release: daskhub
  name: hub
spec:
  ports:
    - name: hub
      port: 8081
      targetPort: http
  selector:
    app: jupyterhub
    component: hub
    release: daskhub
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: proxy-api
    heritage: Helm
    release: daskhub
  name: proxy-api
spec:
  ports:
    - port: 8001
      targetPort: api
  selector:
    app: jupyterhub
    component: proxy
    release: daskhub
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: autohttps
    heritage: Helm
    release: daskhub
  name: proxy-http
spec:
  ports:
    - port: 8000
      targetPort: http
  selector:
    app: jupyterhub
    component: proxy
    release: daskhub
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: proxy-public
    heritage: Helm
    release: daskhub
  name: proxy-public
spec:
  ports:
    - name: https
      port: 443
      targetPort: https
    - name: http
      port: 80
      targetPort: http
  selector:
    component: autohttps
    release: daskhub
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: traefik-daskhub-dask-gateway
spec:
  ports:
    - name: web
      port: 80
      targetPort: 8000
  selector:
    app.kubernetes.io/component: traefik
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/name: dask-gateway
  type: ClusterIP
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: hub
    heritage: Helm
    release: daskhub
  name: hub-db-dir
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: api-daskhub-dask-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: daskhub
      app.kubernetes.io/name: dask-gateway
  template:
    metadata:
      annotations:
        checksum/configmap: 2bd410363e4d95ce912dc6384f7d26239556387bb98548c82205a36b0b697108
      labels:
        app.kubernetes.io/component: gateway
        app.kubernetes.io/instance: daskhub
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: dask-gateway
        app.kubernetes.io/version: 2022.6.1
        gateway.dask.org/instance: daskhub-dask-gateway
        helm.sh/chart: dask-gateway-2022.6.1
        hub.jupyter.org/network-access-hub: 'true'
    spec:
      containers:
        - args:
            - dask-gateway-server
            - --config
            - /etc/dask-gateway/dask_gateway_config.py
          env:
            - name: JUPYTERHUB_API_TOKEN
              valueFrom:
                secretKeyRef:
                  key: jupyterhub-api-token
                  name: api-daskhub-dask-gateway
          image: ghcr.io/dask/dask-gateway-server:2022.6.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /api/health
              port: api
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 2
          name: gateway
          ports:
            - containerPort: 8000
              name: api
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: api
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 2
          volumeMounts:
            - mountPath: /etc/dask-gateway/
              name: configmap
      serviceAccountName: api-daskhub-dask-gateway
      volumes:
        - configMap:
            name: api-daskhub-dask-gateway
          name: configmap
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: autohttps
    heritage: Helm
    kustomization: patch-jupyterhub-autohttps-startup-delay
    release: daskhub
  name: autohttps
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jupyterhub
      component: autohttps
      release: daskhub
  template:
    metadata:
      annotations:
        checksum/static-config: 2c6b369b3ac56cb757248a03ba940643c62cd989b435f34cbcd3d57eb7e5f6e4
      labels:
        app: jupyterhub
        component: autohttps
        hub.jupyter.org/network-access-proxy-http: 'true'
        release: daskhub
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values:
                      - core
              weight: 100
      containers:
        - image: traefik:v2.4.11
          livenessProbe:
            exec:
              command:
                - grep
                - main
                - /etc/acme/acme.json
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 3
          name: traefik
          ports:
            - containerPort: 8080
              name: http
            - containerPort: 8443
              name: https
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
          volumeMounts:
            - mountPath: /etc/traefik
              name: traefik-config
            - mountPath: /etc/acme
              name: certificates
        - args:
            - watch-save
            - --label=app=jupyterhub
            - --label=release=daskhub
            - --label=chart=jupyterhub-1.2.0
            - --label=heritage=secret-sync
            - proxy-public-tls-acme
            - acme.json
            - /etc/acme/acme.json
          env:
            - name: PYTHONUNBUFFERED
              value: 'True'
          image: jupyterhub/k8s-secret-sync:1.2.0
          name: secret-sync
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
          volumeMounts:
            - mountPath: /etc/acme
              name: certificates
      initContainers:
        - command:
            - sh
            - -c
            - sleep 30
          image: busybox:stable
          name: startup-delay
        - args:
            - load
            - proxy-public-tls-acme
            - acme.json
            - /etc/acme/acme.json
          env:
            - name: PYTHONUNBUFFERED
              value: 'True'
          image: jupyterhub/k8s-secret-sync:1.2.0
          name: load-acme
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
          volumeMounts:
            - mountPath: /etc/acme
              name: certificates
      nodeSelector: {}
      serviceAccountName: autohttps
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
      volumes:
        - emptyDir: {}
          name: certificates
        - configMap:
            name: autohttps
          name: traefik-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: controller-daskhub-dask-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: daskhub
      app.kubernetes.io/name: dask-gateway
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/configmap: 658266b53f357a3b93bf409c5cca68f0031b905d3365b0cd4115cb0a1df19f9a
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: daskhub
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: dask-gateway
        app.kubernetes.io/version: 2022.6.1
        gateway.dask.org/instance: daskhub-dask-gateway
        helm.sh/chart: dask-gateway-2022.6.1
    spec:
      containers:
        - args:
            - dask-gateway-server
            - kube-controller
            - --config
            - /etc/dask-gateway/dask_gateway_config.py
          image: ghcr.io/dask/dask-gateway-server:2022.6.1
          imagePullPolicy: IfNotPresent
          name: controller
          ports:
            - containerPort: 8000
              name: api
          volumeMounts:
            - mountPath: /etc/dask-gateway/
              name: configmap
      serviceAccountName: controller-daskhub-dask-gateway
      volumes:
        - configMap:
            name: controller-daskhub-dask-gateway
          name: configmap
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: hub
    heritage: Helm
    release: daskhub
  name: hub
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jupyterhub
      component: hub
      release: daskhub
  strategy:
    type: Recreate
  template:
    metadata:
      annotations: {}
      labels:
        app: jupyterhub
        component: hub
        hub.jupyter.org/network-access-proxy-api: 'true'
        hub.jupyter.org/network-access-proxy-http: 'true'
        hub.jupyter.org/network-access-singleuser: 'true'
        release: daskhub
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values:
                      - core
              weight: 100
      containers:
        - args:
            - jupyterhub
            - --config
            - /usr/local/etc/jupyterhub/jupyterhub_config.py
            - --debug
            - --upgrade-db
          env:
            - name: PYTHONUNBUFFERED
              value: '1'
            - name: HELM_RELEASE_NAME
              value: daskhub
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  key: hub.config.ConfigurableHTTPProxy.auth_token
                  name: hub
            - name: EXTRA_PIP_PACKAGES
              value: boto3 dask_gatway_server
          image: jupyterhub/k8s-hub:1.2.0
          livenessProbe:
            failureThreshold: 30
            httpGet:
              path: /hub/health
              port: http
            initialDelaySeconds: 300
            periodSeconds: 10
            timeoutSeconds: 3
          name: hub
          ports:
            - containerPort: 8081
              name: http
          readinessProbe:
            failureThreshold: 1000
            httpGet:
              path: /hub/health
              port: http
            initialDelaySeconds: 0
            periodSeconds: 2
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 1000
            runAsUser: 1000
          volumeMounts:
            - mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.py
              name: config
              subPath: jupyterhub_config.py
            - mountPath: /usr/local/etc/jupyterhub/z2jh.py
              name: config
              subPath: z2jh.py
            - mountPath: /usr/local/etc/jupyterhub/config/
              name: config
            - mountPath: /usr/local/etc/jupyterhub/secret/
              name: secret
            - mountPath: /srv/jupyterhub
              name: pvc
      nodeSelector:
        lifecycle: OnDemand
      securityContext:
        fsGroup: 1000
      serviceAccountName: hub
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
      volumes:
        - configMap:
            name: hub
          name: config
        - name: secret
          secret:
            secretName: hub
        - name: pvc
          persistentVolumeClaim:
            claimName: hub-db-dir
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: proxy
    heritage: Helm
    release: daskhub
  name: proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jupyterhub
      component: proxy
      release: daskhub
  strategy:
    type: Recreate
  template:
    metadata:
      annotations: {}
      labels:
        app: jupyterhub
        component: proxy
        hub.jupyter.org/network-access-hub: 'true'
        hub.jupyter.org/network-access-singleuser: 'true'
        release: daskhub
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values:
                      - core
              weight: 100
      containers:
        - command:
            - configurable-http-proxy
            - --ip=
            - --api-ip=
            - --api-port=8001
            - --default-target=http://hub:$(HUB_SERVICE_PORT)
            - --error-target=http://hub:$(HUB_SERVICE_PORT)/hub/error
            - --port=8000
            - --log-level=debug
          env:
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  key: hub.config.ConfigurableHTTPProxy.auth_token
                  name: hub
          image: jupyterhub/configurable-http-proxy:4.5.0
          livenessProbe:
            httpGet:
              path: /_chp_healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
          name: chp
          ports:
            - containerPort: 8000
              name: http
            - containerPort: 8001
              name: api
          readinessProbe:
            httpGet:
              path: /_chp_healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 2
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
      nodeSelector:
        lifecycle: OnDemand
      terminationGracePeriodSeconds: 60
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: traefik-daskhub-dask-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: traefik
      app.kubernetes.io/instance: daskhub
      app.kubernetes.io/name: dask-gateway
  template:
    metadata:
      labels:
        app.kubernetes.io/component: traefik
        app.kubernetes.io/instance: daskhub
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: dask-gateway
        app.kubernetes.io/version: 2022.6.1
        gateway.dask.org/instance: daskhub-dask-gateway
        helm.sh/chart: dask-gateway-2022.6.1
    spec:
      containers:
        - args:
            - --global.checknewversion=False
            - --global.sendanonymoususage=False
            - --ping=true
            - --providers.kubernetescrd
            - --providers.kubernetescrd.allowCrossNamespace=true
            - --providers.kubernetescrd.labelselector=gateway.dask.org/instance=daskhub-dask-gateway
            - --providers.kubernetescrd.throttleduration=2
            - --log.level=WARN
            - --entryPoints.traefik.address=:9000
            - --entryPoints.web.address=:8000
          image: traefik:2.6.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
            - containerPort: 9000
              name: traefik
            - containerPort: 8000
              name: web
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          securityContext:
            runAsGroup: 1000
            runAsUser: 1000
      serviceAccountName: traefik-daskhub-dask-gateway
      terminationGracePeriodSeconds: 60
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: user-scheduler
    heritage: Helm
    release: daskhub
  name: user-scheduler
spec:
  replicas: 2
  selector:
    matchLabels:
      app: jupyterhub
      component: user-scheduler
      release: daskhub
  template:
    metadata:
      annotations: {}
      labels:
        app: jupyterhub
        component: user-scheduler
        release: daskhub
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values:
                      - core
              weight: 100
      containers:
        - command:
            - /usr/local/bin/kube-scheduler
            - --config=/etc/user-scheduler/config.yaml
            - --authentication-skip-lookup=true
            - --v=4
          image: k8s.gcr.io/kube-scheduler:v1.22.17
          livenessProbe:
            httpGet:
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
          name: kube-scheduler
          readinessProbe:
            httpGet:
              path: /healthz
              port: 10251
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
          volumeMounts:
            - mountPath: /etc/user-scheduler
              name: config
      nodeSelector: {}
      serviceAccountName: user-scheduler
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
      volumes:
        - configMap:
            name: user-scheduler
          name: config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: user-placeholder
    heritage: Helm
    release: daskhub
  name: user-placeholder
spec:
  podManagementPolicy: Parallel
  replicas: 0
  selector:
    matchLabels:
      app: jupyterhub
      component: user-placeholder
      release: daskhub
  serviceName: user-placeholder
  template:
    metadata:
      labels:
        app: jupyterhub
        component: user-placeholder
        release: daskhub
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values:
                      - user
              weight: 100
      automountServiceAccountToken: false
      containers:
        - image: k8s.gcr.io/pause:3.5
          name: pause
          resources:
            limits:
              cpu: 4
              memory: 4G
            requests:
              cpu: 1
              memory: 1G
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
      nodeSelector: {}
      schedulerName: daskhub-user-scheduler
      terminationGracePeriodSeconds: 0
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: user-placeholder
    heritage: Helm
    release: daskhub
  name: user-placeholder
spec:
  minAvailable: 0
  selector:
    matchLabels:
      app: jupyterhub
      component: user-placeholder
      release: daskhub
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: user-scheduler
    heritage: Helm
    release: daskhub
  name: user-scheduler
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: jupyterhub
      component: user-scheduler
      release: daskhub
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: continuous-image-puller
    heritage: Helm
    release: daskhub
  name: continuous-image-puller
spec:
  selector:
    matchLabels:
      app: jupyterhub
      component: continuous-image-puller
      release: daskhub
  template:
    metadata:
      labels:
        app: jupyterhub
        component: continuous-image-puller
        release: daskhub
    spec:
      automountServiceAccountToken: false
      containers:
        - image: k8s.gcr.io/pause:3.5
          name: pause
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
      initContainers:
        - command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          image: jupyterhub/k8s-network-tools:1.2.0
          name: image-pull-metadata-block
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          image: <GENERIC_DOCKER_LOCATION>:<GENERIC_DOCKER_VERSION>
          name: image-pull-singleuser
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          image: alpine:3
          name: image-pull-singleuser-init-and-extra-containers-0
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          image: public.ecr.aws/q3h7b4o8/helio-science-tutorials:latest
          name: image-pull-singleuser-init-and-extra-containers-1
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          image: alpine:3
          name: image-pull-singleuser-init-and-extra-containers-2
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          image: <ML_DOCKER_LOCATION>:<ML_DOCKER_VERSION>
          name: image-pull-singleuser-profilelist-4
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          image: <ML_DOCKER_LOCATION>:<ML_DOCKER_VERSION>
          name: image-pull-singleuser-profilelist-5
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          image: <ML_DOCKER_LOCATION>:<ML_DOCKER_VERSION>
          name: image-pull-singleuser-profilelist-6
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
      nodeSelector: {}
      terminationGracePeriodSeconds: 0
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 100%
    type: RollingUpdate
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: autohttps
    heritage: Helm
    release: daskhub
  name: autohttps
spec:
  egress:
    - ports:
        - port: 8000
      to:
        - podSelector:
            matchLabels:
              app: jupyterhub
              component: proxy
              release: daskhub
    - ports:
        - port: 53
          protocol: UDP
        - port: 53
          protocol: TCP
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
  ingress:
    - ports:
        - port: http
        - port: https
    - from:
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-proxy-http: 'true'
      ports:
        - port: http
        - port: https
  podSelector:
    matchLabels:
      app: jupyterhub
      component: autohttps
      release: daskhub
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: hub
    heritage: Helm
    release: daskhub
  name: hub
spec:
  egress:
    - ports:
        - port: 8001
      to:
        - podSelector:
            matchLabels:
              app: jupyterhub
              component: proxy
              release: daskhub
    - ports:
        - port: 8888
      to:
        - podSelector:
            matchLabels:
              app: jupyterhub
              component: singleuser-server
              release: daskhub
    - ports:
        - port: 53
          protocol: UDP
        - port: 53
          protocol: TCP
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
  ingress:
    - from:
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-hub: 'true'
      ports:
        - port: http
  podSelector:
    matchLabels:
      app: jupyterhub
      component: hub
      release: daskhub
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: proxy
    heritage: Helm
    release: daskhub
  name: proxy
spec:
  egress:
    - ports:
        - port: 8081
      to:
        - podSelector:
            matchLabels:
              app: jupyterhub
              component: hub
              release: daskhub
    - ports:
        - port: 8888
      to:
        - podSelector:
            matchLabels:
              app: jupyterhub
              component: singleuser-server
              release: daskhub
    - ports:
        - port: 53
          protocol: UDP
        - port: 53
          protocol: TCP
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
  ingress:
    - ports:
        - port: http
        - port: https
    - from:
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-proxy-http: 'true'
      ports:
        - port: http
    - from:
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-proxy-api: 'true'
      ports:
        - port: api
  podSelector:
    matchLabels:
      app: jupyterhub
      component: proxy
      release: daskhub
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app: jupyterhub
    chart: jupyterhub-1.2.0
    component: singleuser
    heritage: Helm
    release: daskhub
  name: singleuser
spec:
  egress:
    - ports:
        - port: 8081
      to:
        - podSelector:
            matchLabels:
              app: jupyterhub
              component: hub
              release: daskhub
    - ports:
        - port: 53
          protocol: UDP
        - port: 53
          protocol: TCP
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              - 169.254.169.254/32
  ingress:
    - from:
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-singleuser: 'true'
      ports:
        - port: notebook-port
  podSelector:
    matchLabels:
      app: jupyterhub
      component: singleuser-server
      release: daskhub
  policyTypes:
    - Ingress
    - Egress
---
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: api-daskhub-dask-gateway
spec:
  entryPoints:
    - web
  routes:
    - kind: Rule
      match: PathPrefix(`/services/dask-gateway`)
      middlewares:
        - name: api-prefix-daskhub-dask-gateway
      services:
        - name: api-daskhub-dask-gateway
          port: 8000
---
apiVersion: traefik.containo.us/v1alpha1
kind: Middleware
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: api-prefix-daskhub-dask-gateway
spec:
  stripPrefix:
    forceSlash: false
    prefixes:
      - /services/dask-gateway
---
apiVersion: traefik.containo.us/v1alpha1
kind: Middleware
metadata:
  labels:
    app.kubernetes.io/instance: daskhub
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/version: 2022.6.1
    gateway.dask.org/instance: daskhub-dask-gateway
    helm.sh/chart: dask-gateway-2022.6.1
  name: clusters-prefix-daskhub-dask-gateway
spec:
  stripPrefixRegex:
    regex:
      - /services/dask-gateway/clusters/[a-zA-Z0-9.-]+
---
# Source: daskhub/charts/jupyterhub/templates/image-puller/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: daskhub
    chart: jupyterhub-1.2.0
    heritage: Helm
    hub.jupyter.org/deletable: 'true'
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: '0'
---
# Source: daskhub/charts/jupyterhub/templates/image-puller/rbac.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: daskhub
    chart: jupyterhub-1.2.0
    heritage: Helm
    hub.jupyter.org/deletable: 'true'
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: '0'
rules:
  - apiGroups: [apps]         # "" indicates the core API group
    resources: [daemonsets]
    verbs: [get]
---
# Source: daskhub/charts/jupyterhub/templates/image-puller/rbac.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: daskhub
    chart: jupyterhub-1.2.0
    heritage: Helm
    hub.jupyter.org/deletable: 'true'
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: '0'
subjects:
  - kind: ServiceAccount
    name: hook-image-awaiter
    namespace: daskhub
roleRef:
  kind: Role
  name: hook-image-awaiter
  apiGroup: rbac.authorization.k8s.io
---
# Source: daskhub/charts/jupyterhub/templates/image-puller/daemonset-hook.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: hook-image-puller
  labels:
    component: hook-image-puller
    app: jupyterhub
    release: daskhub
    chart: jupyterhub-1.2.0
    heritage: Helm
    hub.jupyter.org/deletable: 'true'
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: '-10'
spec:
  selector:
    matchLabels:
      component: hook-image-puller
      app: jupyterhub
      release: daskhub
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 100%
  template:
    metadata:
      labels:
        component: hook-image-puller
        app: jupyterhub
        release: daskhub
    spec:
      nodeSelector: {}
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      initContainers:
        - name: image-pull-metadata-block
          image: jupyterhub/k8s-network-tools:1.2.0
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - name: image-pull-singleuser
          image: <GENERIC_DOCKER_LOCATION>:<GENERIC_DOCKER_VERSION>
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - name: image-pull-singleuser-init-and-extra-containers-0
          image: alpine:3
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - name: image-pull-singleuser-init-and-extra-containers-1
          image: public.ecr.aws/q3h7b4o8/helio-science-tutorials:latest
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - name: image-pull-singleuser-init-and-extra-containers-2
          image: alpine:3
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - name: image-pull-singleuser-profilelist-4
          image: <ML_DOCKER_LOCATION>:<ML_DOCKER_VERSION>
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - name: image-pull-singleuser-profilelist-5
          image: <ML_DOCKER_LOCATION>:<ML_DOCKER_VERSION>
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
        - name: image-pull-singleuser-profilelist-6
          image: <ML_DOCKER_LOCATION>:<ML_DOCKER_VERSION>
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
      containers:
        - name: pause
          image: k8s.gcr.io/pause:3.5
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
---
# Source: daskhub/charts/jupyterhub/templates/image-puller/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: daskhub
    chart: jupyterhub-1.2.0
    heritage: Helm
    hub.jupyter.org/deletable: 'true'
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: '10'
spec:
  template:
    # The hook-image-awaiter Job and hook-image-puller DaemonSet was
    # conditionally created based on this state:
    #
    # prePuller.hook.enabled=true
    # prePuller.hook.pullOnlyOnChanges=true
    # post-upgrade checksum != pre-upgrade checksum (of the hook-image-puller DaemonSet)
    # "174f263aebb394d40e11a17a05200a37bf2ce7f0313579b5281bf6b43fd4a2be" != ""
    #
    metadata:
      labels:
        component: image-puller
        app: jupyterhub
        release: daskhub
    spec:
      restartPolicy: Never
      serviceAccountName: hook-image-awaiter
      nodeSelector: {}
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
      containers:
        - image: jupyterhub/k8s-image-awaiter:1.2.0
          name: hook-image-awaiter
          command:
            - /image-awaiter
            - -ca-path=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            - -auth-token-path=/var/run/secrets/kubernetes.io/serviceaccount/token
            - -api-server-address=https://kubernetes.default.svc:$(KUBERNETES_SERVICE_PORT)
            - -namespace=daskhub
            - -daemonset=hook-image-puller
            - -pod-scheduling-wait-duration=10
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
