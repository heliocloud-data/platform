apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: eks-helio
  region: <INSERT_REGION>
  version: "1.30"
  tags:
      Product: heliocloud-daskhub
      Project: heliocloud
secretsEncryption:
    keyARN: <INSERT_KMS_ARN>


availabilityZones: [<INSERT_PRIMARY_AVAILABILITY_ZONE>,<INSERT_SECONDARY_AVAILABILITY_ZONE>]

iam:
  withOIDC: true
  serviceAccounts:
  - metadata:
      name: helio-dh-role
      labels: {aws-usage: "application"}
    attachPolicyARNs:
    - "<INSERT_helio-s3-policy_ARN>"
  - metadata:
      name: helio-dh-role
      namespace: daskhub
    attachPolicyARNs:
    - "<INSERT_helio-s3-policy_ARN>"
  - metadata:
      name: cluster-autoscaler
      namespace: kube-system
    attachPolicyARNs:
    - "<INSERT_k8s-asg-policy_ARN>" 
  - metadata:
      name: admin
      namespace: kube-system
    attachPolicyARNs:
    - "<INSERT_k8s-asg-policy_ARN>"
  - metadata:
      name: ebs-csi-controller-sa
      namespace: kube-system
    attachPolicyARNs: 
    - "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
    roleOnly: true
    roleName: "AmazonEKS_EBS_CSI_DriverRole-us-east-2-eks-helio"

nodeGroups:
  # This node group is responsible for hosting the "user" jupyterhub servers that have
  # GPUs.  Schedule on can is done via the following:
  #
  #  taint/toleration(s)
  #   * nvidia.com/gpu:=true:NoSchedule
  #   * hub.jupyter.org/dedicated=user:NoSchedule
  - name: ng-user-gpu
    instanceType: mixed
    desiredCapacity: 0
    minSize: 0
    maxSize: 4
    instancesDistribution:
      instanceTypes: ["g4dn.4xlarge"]
    labels:
      lifecycle: OnDemand
      intent: apps
      nvidia.com/gpu: 'true'
      k8s.amazonaws.com/accelerator: nvidia-tesla-t4
      node-purpose: user
      hub.jupyter.org/node-purpose: user
    taints:
      nvidia.com/gpu: "true:NoSchedule"
      hub.jupyter.org/dedicated: "user:NoSchedule"
    tags:
      k8s.io/cluster-autoscaler/node-template/label/lifecycle: OnDemand
      k8s.io/cluster-autoscaler/node-template/label/intent: apps
      k8s.io/cluster-autoscaler/node-template/label/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/taint/dedicated: nvidia.com/gpu=true
      k8s.io/cluster-autoscaler/node-template/label/nvidia.com/gpu: 'true'
      k8s.io/cluster-autoscaler/node-template/label/k8s.amazonaws.com/accelerator: nvidia-tesla-t4
    availabilityZones: [<INSERT_PRIMARY_AVAILABILITY_ZONE>]
    iam:
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AmazonRoute53AutoNamingRegistrantAccess
      withAddonPolicies:
        autoScaler: true
        ebs: true
        fsx: true
        efs: true
        awsLoadBalancerController: true
        cloudWatch: true

  # This node group is responsible for hosting the burst jobs "user" created by "dask" jobs.
  # At the moment it's not entirely clear why this node group is selected, as historically
  # no affinity/anti-affinity settings are applied to task.  Theoretically, is can be schedule
  # via the following:
  #
  #  affinity (any)
  #   * lifecycle=Ec2Spot
  #   * aws.amazon.com/spot=true
  - name: ng-burst-compute-spot
    minSize: 0
    maxSize: 10
    desiredCapacity: 0
    availabilityZones: [<INSERT_PRIMARY_AVAILABILITY_ZONE>]
    instancesDistribution:
      instanceTypes: ["m5n.8xlarge", "m5.8xlarge", "m4.8xlarge", "m5dn.8xlarge", "r5n.8xlarge", "r5dn.8xlarge"]
      onDemandBaseCapacity: 1
      onDemandPercentageAboveBaseCapacity: 0
      spotAllocationStrategy: lowest-price
    labels:
      lifecycle: Ec2Spot
      intent: apps
      aws.amazon.com/spot: "true"
      node-purpose: user
      hub.jupyter.org/node-purpose: user
    tags:
      k8s.io/cluster-autoscaler/node-template/label/lifecycle: Ec2Spot
      k8s.io/cluster-autoscaler/node-template/label/intent: apps
      k8s.io/cluster-autoscaler/node-template/label/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
    iam:
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AmazonRoute53AutoNamingRegistrantAccess
      withAddonPolicies:
        autoScaler: true
        ebs: true
        fsx: true
        efs: true
        awsLoadBalancerController: true
        cloudWatch: true

managedNodeGroups:
  # This node group is responsible for hosting the "core" jupyterhub/dask services as well
  # any control plane/infrastructure related services.  For this reason, their can be no
  # taints as we want to use this node group as the "fallback".
  - name: ng-daskhub-services
    instanceType: t3a.medium
    minSize: 2
    maxSize: 3
    desiredCapacity: 2
    availabilityZones: [<INSERT_PRIMARY_AVAILABILITY_ZONE>]
    updateConfig:
      maxUnavailable: 1
    labels:
      lifecycle: OnDemand
      hub.jupyter.org/node-purpose: core
    tags:
      k8s.io/cluster-autoscaler/node-template/label/lifecycle: OnDemand
      k8s.io/cluster-autoscaler/node-template/label/node-purpose: core
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: core
    iam:
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AmazonRoute53AutoNamingRegistrantAccess
      withAddonPolicies:
        autoScaler: true
        ebs: true
        fsx: true
        efs: true
        awsLoadBalancerController: true
        cloudWatch: true

  # This node group is responsible for hosting the "user" jupyterhub servers that are
  # small.  Schedule on can is done via the following:
  #
  #  taint/toleration(s)
  #   * hub.jupyter.org/dedicated=user:NoSchedule
  - name: mng-user-compute
    instanceType: m5.2xlarge
    minSize: 0
    maxSize: 15
    desiredCapacity: 0
    availabilityZones: [<INSERT_PRIMARY_AVAILABILITY_ZONE>]
    labels:
      lifecycle: OnDemand
      intent: apps
      node-purpose: user
      hub.jupyter.org/node-purpose: user
    taints:
    - key: "hub.jupyter.org/dedicated"
      value: "user"
      effect: "NoSchedule"
    tags:
      k8s.io/cluster-autoscaler/node-template/label/lifecycle: OnDemand
      k8s.io/cluster-autoscaler/node-template/label/intent: apps
      k8s.io/cluster-autoscaler/node-template/label/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org/dedicated: 'user:NoSchedule'
    iam:
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AmazonRoute53AutoNamingRegistrantAccess
      withAddonPolicies:
        autoScaler: true
        ebs: true
        fsx: true
        efs: true
        awsLoadBalancerController: true
        cloudWatch: true

  # This node group is responsible for hosting the "user" jupyterhub servers that are
  # large.  Schedule on can is done via the following:
  #
  #  taint/toleration(s)
  #   * hub.jupyter.org/dedicated=big-user:NoSchedule
  - name: mng-user-compute-big
    instanceType: m5.4xlarge
    minSize: 0
    maxSize: 15
    desiredCapacity: 0
    availabilityZones: [<INSERT_PRIMARY_AVAILABILITY_ZONE>]
    updateConfig:
      maxUnavailable: 15
    labels:
      lifecycle: OnDemand
      intent: apps
      node-purpose: big-user
      hub.jupyter.org/node-purpose: big-user
    taints:
    - key: "hub.jupyter.org/dedicated"
      value: "big-user"
      effect: "NoSchedule"
    tags:
      k8s.io/cluster-autoscaler/node-template/label/lifecycle: OnDemand
      k8s.io/cluster-autoscaler/node-template/label/intent: apps
      k8s.io/cluster-autoscaler/node-template/label/node-purpose: big-user
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: big-user
      k8s.io/cluster-autoscaler/node-template/taint/hub.jupyter.org/dedicated: 'big-user:NoSchedule'
    iam:
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AmazonRoute53AutoNamingRegistrantAccess
      withAddonPolicies:
        autoScaler: true
        ebs: true
        fsx: true
        efs: true
        awsLoadBalancerController: true
        cloudWatch: true

cloudWatch:
  clusterLogging:
    enableTypes: ["*"]

addons:
  - name: aws-ebs-csi-driver
    version: v1.31.0-eksbuild.1
    serviceAccountRoleARN: arn:aws:iam:::role/AmazonEKS_EBS_CSI_DriverRole-us-east-2-eks-helio
